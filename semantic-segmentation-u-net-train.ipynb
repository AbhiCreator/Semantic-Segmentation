{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install albumentations\n# !pip install --upgrade opencv-python\n# !pip install opencv-python-headless==4.1.2.30\n# !pip install albumentations==0.4.6","metadata":{"id":"iKCFRdvv92BB","outputId":"45d55c4f-5da4-4539-c4e3-d34fdd1054c4","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-06T12:37:58.128798Z","iopub.execute_input":"2022-07-06T12:37:58.129081Z","iopub.status.idle":"2022-07-06T12:37:58.134018Z","shell.execute_reply.started":"2022-07-06T12:37:58.129049Z","shell.execute_reply":"2022-07-06T12:37:58.132816Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as T\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"id":"SqAJC6ApNVt6","execution":{"iopub.status.busy":"2022-07-06T12:37:58.140193Z","iopub.execute_input":"2022-07-06T12:37:58.140488Z","iopub.status.idle":"2022-07-06T12:37:58.148224Z","shell.execute_reply.started":"2022-07-06T12:37:58.140445Z","shell.execute_reply":"2022-07-06T12:37:58.146943Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nepochs = 50\npath = '../input/semantic-segmentation-of-underwater-imagery-suim/train_val'\nimage_size = 572","metadata":{"id":"amg5l125Qgxn","execution":{"iopub.status.busy":"2022-07-06T12:37:58.152746Z","iopub.execute_input":"2022-07-06T12:37:58.153480Z","iopub.status.idle":"2022-07-06T12:37:58.160743Z","shell.execute_reply.started":"2022-07-06T12:37:58.153429Z","shell.execute_reply":"2022-07-06T12:37:58.159645Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class SUIM(torch.utils.data.Dataset):\n  def __init__(self, datapath, transform, image_size):\n    self.datapath = datapath\n    self.img_paths = glob(self.datapath + \"/images/*.jpg\")\n    self.transform = transform\n    self.size = image_size\n\n  def __len__(self):\n    return len(self.img_paths)\n\n  def __getitem__(self, idx):\n    img_path = self.img_paths[idx]\n    mask_path = self.datapath + \"/masks/\" + img_path.split('/')[-1][:-3] + 'bmp'\n    image = cv2.imread(img_path)\n    dataset_mask = cv2.imread(mask_path)\n    transformed = self.transform(image=image,mask=dataset_mask)\n    mask = self.gen_mask(transformed['mask'])\n    return transformed['image'] * 1.0, mask\n  \n  def gen_mask(self,dataset_mask):\n    dataset_mask = T.functional.center_crop(dataset_mask.permute(2,1,0),\n                                           (388,388))                           \n    mod_mask = (dataset_mask > 100)                                     \n    mask = np.dot(mod_mask.permute(1,2,0).numpy(), [4,2,1]).astype(np.uint8)\n    return torch.Tensor(mask)","metadata":{"id":"V2aK4KF2W43n","execution":{"iopub.status.busy":"2022-07-06T12:37:58.165336Z","iopub.execute_input":"2022-07-06T12:37:58.165970Z","iopub.status.idle":"2022-07-06T12:37:58.177861Z","shell.execute_reply.started":"2022-07-06T12:37:58.165922Z","shell.execute_reply":"2022-07-06T12:37:58.176487Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"dataset = SUIM(datapath=path,\n               transform= A.Compose([\n                   A.PadIfNeeded(min_height=image_size, min_width=image_size,p=1),\n                   A.CenterCrop(height=image_size,width=image_size,p=1),\n                   ToTensorV2()]),\n               image_size=image_size)\n\ntrain_data, test_data = torch.utils.data.random_split(\n    dataset,\n    [1220, 305],\n    generator=torch.Generator().manual_seed(42)\n    )\ntrain_loader = DataLoader(train_data, batch_size=batch_size,\n                          shuffle=True, num_workers=0,\n                          drop_last=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size,\n                          shuffle=False, num_workers=0,\n                          drop_last=False)","metadata":{"id":"YH-toVrmixgt","execution":{"iopub.status.busy":"2022-07-06T12:37:58.179968Z","iopub.execute_input":"2022-07-06T12:37:58.180638Z","iopub.status.idle":"2022-07-06T12:37:58.203050Z","shell.execute_reply.started":"2022-07-06T12:37:58.180591Z","shell.execute_reply":"2022-07-06T12:37:58.201873Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for data, mask in train_loader:\n    data1 = data[0].permute(2,1,0).long().numpy()\n    plt.imshow(data1[:,:,[2,1,0]])\n    plt.show()\n    for i in range(8):\n        plt.imshow((mask == i)[0].numpy() * 255)\n        plt.show()\n    break","metadata":{"id":"ebw0669792BD","outputId":"121688dd-914c-4c08-f55c-6d2e09a68380","execution":{"iopub.status.busy":"2022-07-06T12:37:58.209554Z","iopub.execute_input":"2022-07-06T12:37:58.210254Z","iopub.status.idle":"2022-07-06T12:38:00.620134Z","shell.execute_reply.started":"2022-07-06T12:37:58.210221Z","shell.execute_reply":"2022-07-06T12:38:00.619166Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.conv11 = nn.Conv2d(3,64,3)\n        self.conv12 = nn.Conv2d(64,64,3)\n        self.max_pool2x2 = nn.MaxPool2d(2)\n        self.conv21 = nn.Conv2d(64,128,3)\n        self.conv22 = nn.Conv2d(128,128,3)\n        self.conv31 = nn.Conv2d(128,256,3)\n        self.conv32 = nn.Conv2d(256,256,3)\n        self.conv41 = nn.Conv2d(256,512,3)\n        self.conv42 = nn.Conv2d(512,512,3)\n        self.conv51 = nn.Conv2d(512,1024,3)\n        self.conv52 = nn.Conv2d(1024, 1024,3)\n        self.up6 = nn.ConvTranspose2d(1024,512,2,2)\n        self.conv61 = nn.Conv2d(1024,512,3)\n        self.conv62 = nn.Conv2d(512,512,3)\n        self.up7 = nn.ConvTranspose2d(512,256,2,2)\n        self.conv71 = nn.Conv2d(512,256,3)\n        self.conv72 = nn.Conv2d(256,256,3)\n        self.up8 = nn.ConvTranspose2d(256,128,2,2)\n        self.conv81 = nn.Conv2d(256,128,3)\n        self.conv82 = nn.Conv2d(128,128,3)\n        self.up9 = nn.ConvTranspose2d(128,64,2,2)\n        self.conv91 = nn.Conv2d(128,64,3)\n        self.conv92 = nn.Conv2d(64,64,3)\n        self.conv_final = nn.Conv2d(64,8,1)\n\n    def forward(self, x):\n        x1 = self.conv12(self.conv11(x))\n        x2 = self.conv22(self.conv21(self.max_pool2x2(x1)))\n        x3 = self.conv32(self.conv31(self.max_pool2x2(x2)))\n        x4 = self.conv42(self.conv41(self.max_pool2x2(x3)))\n        x = self.conv52(self.conv51(self.max_pool2x2(x4)))\n        x = torch.cat((self.up6(x), T.functional.center_crop(x4, (56,56))), 1)\n        x = self.conv62(self.conv61(x))\n        x = torch.cat((self.up7(x), T.functional.center_crop(x3, (104,104))),1)\n        x = self.conv72(self.conv71(x))\n        x = torch.cat((self.up8(x), T.functional.center_crop(x2, (200,200))),1)\n        x = self.conv82(self.conv81(x))\n        x = torch.cat((self.up9(x), T.functional.center_crop(x1, (392,392))),1)\n        x = self.conv92(self.conv91(x))\n        x = self.conv_final(x)\n\n        return x","metadata":{"id":"TdTX4lKH2vMy","execution":{"iopub.status.busy":"2022-07-06T12:38:00.623284Z","iopub.execute_input":"2022-07-06T12:38:00.623606Z","iopub.status.idle":"2022-07-06T12:38:00.645679Z","shell.execute_reply.started":"2022-07-06T12:38:00.623568Z","shell.execute_reply":"2022-07-06T12:38:00.644629Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nThe following training loop goes on for 50 epochs","metadata":{"id":"jz52459d92BE"}},{"cell_type":"code","source":"model = UNet().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr = 1e-4)\ncriterion = torch.nn.CrossEntropyLoss()","metadata":{"id":"i-R9jq_PmrIb","execution":{"iopub.status.busy":"2022-07-06T12:38:00.648728Z","iopub.execute_input":"2022-07-06T12:38:00.649166Z","iopub.status.idle":"2022-07-06T12:38:04.523874Z","shell.execute_reply.started":"2022-07-06T12:38:00.649055Z","shell.execute_reply":"2022-07-06T12:38:04.522824Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_losses, test_losses = [], []\ntrain_accs, test_accs = [], []\n\nfor epoch in tqdm(range(epochs)):\n    model.train()\n    train_loss = 0\n    train_correct, train_total = 0,0\n    for idx, (data, mask) in enumerate(tqdm(train_loader)):\n        data, mask = data.to(device), mask.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, mask.long())\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        pred_mask = output.argmax(dim=1)\n        train_correct += (mask == pred_mask).sum()\n        train_total += batch_size * 388 * 388\n\n    train_loss /= len(train_loader)\n    train_acc = (100.0 * train_correct / train_total).item()\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    \n    print(\"Training %d / 50: Loss = %.4f, Accuracy = %2.2f%%\" % (epoch, train_loss, train_acc))\n\n    model.eval()\n    test_loss = 0\n    test_correct, test_total = 0,0\n    with torch.no_grad():\n        for idx, (data, mask) in enumerate(tqdm(test_loader)):\n            data, mask = data.to(device), mask.to(device)\n            output = model(data)\n            loss = criterion(output, mask.long())\n            test_loss = loss.item()\n            pred_mask = output.argmax(dim=1)\n            test_correct += (mask == pred_mask).sum()\n            test_total += batch_size * 388 * 388\n\n    test_loss /= len(test_loader)\n    test_acc = (100.0 * test_correct / test_total).item()\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    print(\"Testing Epoch %d / 50: Loss = %.4f, Accuracy = %2.2f%%\" % (epoch, test_loss, test_acc))","metadata":{"id":"KrSTgwHnh0JF","outputId":"7bcd098c-fd21-4ab3-f54b-1be548934114","execution":{"iopub.status.busy":"2022-07-06T12:38:04.526930Z","iopub.execute_input":"2022-07-06T12:38:04.527339Z","iopub.status.idle":"2022-07-06T14:44:26.687379Z","shell.execute_reply.started":"2022-07-06T12:38:04.527291Z","shell.execute_reply":"2022-07-06T14:44:26.686433Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Plots\n\nBelow we generate training and validation plots using matplotlib for accuracy and loss.","metadata":{"id":"Xd4auLW492BF"}},{"cell_type":"code","source":"# Plot generation for training and testing loss CIFAR-10\n\nplt.plot(train_losses, color='green', label=\"Train Loss\")\nplt.plot(test_losses, color='orange', label=\"Val Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('SUIM Training and Validation Loss')\n\nplt.show()","metadata":{"id":"qupBs3Kz92BF","execution":{"iopub.status.busy":"2022-07-06T14:44:26.689253Z","iopub.execute_input":"2022-07-06T14:44:26.690261Z","iopub.status.idle":"2022-07-06T14:44:26.917405Z","shell.execute_reply.started":"2022-07-06T14:44:26.690215Z","shell.execute_reply":"2022-07-06T14:44:26.916378Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Plot generation for training and testing accuracy CIFAR-10\n\nplt.plot(train_accs, color='green', label=\"Train Accuracy\")\nplt.plot(test_accs, color='orange', label=\"Val Accuracy\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('SUIM Training and Validation Accuracies')\n\nplt.show()","metadata":{"id":"fstOtk9_92BG","execution":{"iopub.status.busy":"2022-07-06T14:44:26.919318Z","iopub.execute_input":"2022-07-06T14:44:26.919574Z","iopub.status.idle":"2022-07-06T14:44:27.150512Z","shell.execute_reply.started":"2022-07-06T14:44:26.919545Z","shell.execute_reply":"2022-07-06T14:44:27.149587Z"},"trusted":true},"execution_count":22,"outputs":[]}]}